\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Pix2Mask2Pix: CLIP Guided Text-to-Mask-to-Image Editing}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Minyang Xie\\
  IIIS\\
  Tsinghua University\\
  % \texttt{xiemy23@mails.tsinghua.edu.cn} \\
  % examples of more authors
  \And
  Jiayi Hu \\
  IIIS \\
  Tsinghua University \\
  % \texttt{hu-jy23@mails.tsinghua.edu.cn} \\
  \And
  Jiani Wang \\
  IIIS \\
  Tsinghua University \\
  % \texttt{wangjn23@mails.tsinghua.edu.cn} \\
}


\begin{document}


\maketitle


\begin{abstract}
We present \emph{Pix2Mask2Pix}, a pipeline for text guided image editing. A
CLIP based segmenter predicts the region to modify from a prompt while the user
can optionally provide their own mask. The masked area is described by a
vision--language model and then completed using a diffusion based inpainting
model. Compared with Pix2Pix, our approach better preserves the original image
style and supports free form edits without paired training data.
\end{abstract}


\section{Introduction}


Recent advances in generative modeling allow images to be modified through
natural language commands. Such functionality is attractive for creative work
and casual photo editing. Early systems like Pix2Pix learn paired mappings but
remain limited to their training domains and mask shapes. Modern large models,
including GPT-4o and Stable Diffusion, can synthesise diverse imagery but lack
precise control over the region to edit.

We propose a modular pipeline that first predicts a mask using a CLIP-based
segmenter. Users may refine this mask manually before a vision--language model
describes the masked content. A diffusion inpainter then generates the final
pixels guided by this description. This approach preserves the original style
while supporting arbitrary masks beyond the reach of Pix2Pix. We further
fine-tune the mask predictor on the Magic Brush dataset so that segmentations
better align with textual prompts.


\section{Related Work}


\subsection{Pix2Pix}

Pix2Pix~\cite{isola2017pix2pix} introduced conditional adversarial networks for
image-to-image translation. The model learns a mapping from an input image to a
target output using paired data and a discriminator to encourage realism.
Although effective for tasks like facade generation or sketch colouring, it
requires aligned training pairs and assumes that the structure of the output is
similar to the input. As a result Pix2Pix struggles with arbitrary mask shapes
and complex modifications that fall outside of its training distribution.


\subsection{Text guided Image Embedding}


Large scale vision--language models bridge textual concepts and visual content.
CLIP~\cite{radford2021clip} and later variants like SigCLIP learn joint
embeddings for text and images, enabling retrieval and localisation of objects
described by language. By projecting image patches and text into a shared space,
these models can produce masks corresponding to user prompts. Such masks provide
a strong prior for editing tasks and have been used to condition diffusion-based
image synthesis.


\subsection{Text guided Image Inpainting}


Inpainting aims to synthesise plausible content in masked regions while keeping
the surroundings consistent. Diffusion-based methods such as Stable
Diffusion~\cite{rombach2022stable} can be conditioned on a text prompt together
with a spatial mask to edit only the selected area. This allows high quality
text driven modifications without retraining the generator.


\subsection{GPT-4o}


The recently released GPT-4o model combines language and vision abilities in a
single architecture. It can follow multimodal instructions to produce images or
perform simple inpainting. However the system is closed source and offers little
control over the mask or the preservation of fine-grained style. In contrast our
pipeline relies on open models where each stage can be tuned to specific editing
requirements.


\section{Pipeline Implementation}


\subsection{Overview}


The overall workflow consists of three sequential stages as illustrated in
Figure~\ref{fig:pipeline}:


\begin{itemize}
    \item User input text prompt and/or mask
    \item Use CLIP to generate a mask from the text prompt
    \item Use Qwen to generate a description of the mask
    \item Use Stable Diffusion to inpaint the image from the mask
\end{itemize}


\subsection{CLIP Mask Generation}


We adopt SlipSeg, a CLIP-based segmentation model, to estimate the region
referred to by a textual prompt. The image features are compared with the prompt
embedding to produce a similarity map, which is then thresholded into a binary
mask. Users may further edit this mask before the description stage.


\subsection{Qwen Description}


Once the mask is obtained we generate a concise description of the region with
Qwen-VL, an open vision--language model. The model summarises the masked area in
a short phrase (e.g., ``red umbrella'') which is then fed into the diffusion
inpainter. These semantic hints help steer generation toward the intended object
or style while keeping the rest of the scene intact.


\subsection{Stable Diffusion Inpainting}


Finally we employ Stable Diffusion's inpainting model to synthesise new content
inside the masked area. The model takes the original image, the binary mask and
the description generated by Qwen as inputs. Conditioning on both the visual
context and the textual hint enables coherent completion that blends smoothly
with the unmodified regions.


\section{Experiments and Training}


We probably need to train the CLIP model on our Magic Brush dataset to improve the mask generation quality. I will upload the training script and how to download the dataset.


We need also think of a metric (maybe miou to verify the mask quality, K-means to verify the mask clustering quality, and FID to verify the image quality) to evaluate the performance of our pipeline.


Also, I have tried to finetune the Pix2Pix model on our Magic Brush dataset, and I will paste a few cherry picked results here.


\section{Results}


Show a few paired results of our pipeline. You may compare with the results of Pix2Pix and GPT-4o.


\section{Ablation Study}


\subsection{CLIP Mask Generation}


Get rid of the CLIP mask generation step, do full-inpainting with Stable Diffusion, and compare the results with the full pipeline.


\subsection{Old / New CLIP model}


Change the CLIP model to the old one, and compare the results with the full pipeline.


\subsection{Qwen Description}


Get rid of the Qwen description step, and use the CLIP mask directly to inpaint the image. Compare the results with the full pipeline.


\section{Discussion}


While the proposed approach offers more control than prior work, several
limitations remain. The CLIP-based segmentation sometimes produces coarse masks
that miss fine details. The textual hints generated by Qwen may also be
ambiguous, leading to inconsistent inpainting. Finally, diffusion models still
struggle with complicated structures and text rendering. Future extensions could
explore stronger segmentation models and tighter integration between the
language and diffusion components.


\section{Conclusion}


In this report, we have presented a pipeline that can complete the text-to-image editing task by first generating a mask from the text prompt and then inpainting the image from the mask. We have shown that our pipeline can achieve better results than the existing Pix2Pix model, and we have also allowed for user drawn masks. Also described the training work we have done to improve the mask generation quality.




\end{document}