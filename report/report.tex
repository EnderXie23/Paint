\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Pix2Mask2Pix: CLIP Guided Text-to-Mask-to-Image Editing}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Minyang Xie\\
  IIIS\\
  Tsinghua University\\
  % \texttt{xiemy23@mails.tsinghua.edu.cn} \\
  % examples of more authors
  \And
  Jiayi Hu \\
  IIIS \\
  Tsinghua University \\
  % \texttt{hu-jy23@mails.tsinghua.edu.cn} \\
  \And
  Jiani Wang \\
  IIIS \\
  Tsinghua University \\
  % \texttt{wangjn23@mails.tsinghua.edu.cn} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Write an abstract, stating that we have achieved a pipeline that can complete the text-to-image editing task by first generating a mask from the text prompt and then inpainting the image from the mask. We also allow for user drawn mask. Compare the pipeline with the existing Pix2Pix model.
\end{abstract}


\section{Introduction}


Describe the needs / usages for text guided image editing. Especially the results attained by pix2pix and GPT-4o model series. Then \textbf{briefly introduce} the pipeline we have, pointing out the advantages of our pipeline over the existing models (style-keeping, self-drawn mask, etc.). Finally, we shall describe the training work we have done.


\section{Related Work}


\subsection{Pix2Pix}


Add related work on Pix2Pix, show pictures, cite the original paper, and describe the model architecture. Also, mention the limitations of Pix2Pix, such as the difficulty in generating complex masks.


\subsection{Text guided Image Embedding}


Add related work on text guided image editing, such as CLIP, SigCLIP etc. Describe how these models can be used to generate masks from text prompts, and how they can be used to guide the image editing process.


\subsection{Text guided Image Inpainting}


Cite Stable Diffusion models.


\subsection{GPT-4o}


Add a photo descibing the GPT-4o image generation tool, and add a few inpainting examples.


\section{Pipeline Implementation}


\subsection{Overview}


Plot a flowchart here to show the pipeline we have implemented:


\begin{itemize}
    \item User input text prompt and/or mask
    \item Use CLIP to generate a mask from the text prompt
    \item Use Qwen to generate a description of the mask
    \item Use Stable Diffusion to inpaint the image from the mask
\end{itemize}


\subsection{CLIP Mask Generation}


Describe how we use CLIP to generate a mask from the text prompt. Show some examples of the masks generated by CLIP.


\subsection{Qwen Description}


Descibe why we need to use Qwen to generate a description of the mask, and how we use it to guide the image inpainting process. Show some examples of the descriptions generated by Qwen and images generated.


\subsection{Stable Diffusion Inpainting}


Describe how we use Stable Diffusion to inpaint the image from the mask. Show some examples of the inpainted images.


\section{Experiments and Training}


We probably need to train the CLIP model on our Magic Brush dataset to improve the mask generation quality. I will upload the training script and how to download the dataset.


We need also think of a metric (maybe miou to verify the mask quality, K-means to verify the mask clustering quality, and FID to verify the image quality) to evaluate the performance of our pipeline.


Also, I have tried to finetune the Pix2Pix model on our Magic Brush dataset, and I will paste a few cherry picked results here.


\section{Results}


Show a few paired results of our pipeline. You may compare with the results of Pix2Pix and GPT-4o.


\section{Ablation Study}


\subsection{CLIP Mask Generation}


Get rid of the CLIP mask generation step, do full-inpainting with Stable Diffusion, and compare the results with the full pipeline.


\subsection{Old / New CLIP model}


Change the CLIP model to the old one, and compare the results with the full pipeline.


\subsection{Qwen Description}


Get rid of the Qwen description step, and use the CLIP mask directly to inpaint the image. Compare the results with the full pipeline.


\section{Discussion}


Some of the limitations of our pipeline, such as the quality of the masks generated by CLIP, the guide strength of the Qwen description, and the quality of the inpainted images. Also, compare with SOTA transformer structured models, note the ability to generate complex masks and drawing text.


\section{Conclusion}


In this report, we have presented a pipeline that can complete the text-to-image editing task by first generating a mask from the text prompt and then inpainting the image from the mask. We have shown that our pipeline can achieve better results than the existing Pix2Pix model, and we have also allowed for user drawn masks. Also described the training work we have done to improve the mask generation quality.




\end{document}